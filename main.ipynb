{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# EfficientNetV2: Smaller Models and Faster Training <br>\n",
        "In this project, we study the family of convolutional networks, called *EfficientNetV2*, presented in the paper: <br>\n",
        "\n",
        "''Tan, Mingxing, and Quoc Le. \"Efficientnetv2: Smaller models and faster training.\" In International Conference on Machine Learning, pp. 10096-10106. PMLR, 2021.'',\n",
        "\n",
        "and explore its features and advantages over the existing methods in the literature. \n"
      ],
      "metadata": {
        "id": "MnyGMJbYRV3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup:** <br>\n",
        "The version of the packages required for running the codes:<br>\n",
        "python 3.7.7, cuda 10.1, pytorch 1.11.0, timm 0.5.4\n"
      ],
      "metadata": {
        "id": "MthEM9j4fwa1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cazMzvoiSKBc",
        "outputId": "dafd3d7d-b0ba-47b5-ad92-2d725b88808b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->timm) (2021.10.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm \n",
        "\n",
        "netv2_s = timm.create_model('efficientnetv2_rw_s', pretrained = True)\n",
        "# print(\"Architecture of EfficientNetV2-S: \", netv2_s.eval())\n",
        "netv2_s_params = sum([m.numel() for m in netv2_s.parameters()])\n",
        "print(\"EfficientNetV2-S Params: \", netv2_s_params)\n",
        "print(\"EfficientNetV2-S Classifier: \", netv2_s.get_classifier())\n",
        "netv2_m = timm.create_model('efficientnetv2_rw_m', pretrained = True)\n",
        "netv2_m_params = sum([m.numel() for m in netv2_m.parameters()])\n",
        "print(\"EfficientNetV2-M Params: \", netv2_m_params)\n",
        "print(\"EfficientNetV2-M Classifier: \", netv2_m.get_classifier())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VFqOtC3Uo09",
        "outputId": "dbe144a9-57dc-4979-ad6f-944f8dfd3cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EfficientNetV2-S Params:  23941296\n",
            "EfficientNetV2-S Classifier:  Linear(in_features=1792, out_features=1000, bias=True)\n",
            "EfficientNetV2-M Params:  53236442\n",
            "EfficientNetV2-M Classifier:  Linear(in_features=2152, out_features=1000, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py \n",
        "\n",
        "################################################################################\n",
        "### IMPORTS AND INITIAL SETUP\n",
        "################################################################################\n",
        "\n",
        "import argparse, time, os, logging, matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "from contextlib import suppress\n",
        "from datetime import datetime\n",
        "import torch, torch.nn as nn, torchvision.utils\n",
        "from torch.nn.parallel import DistributedDataParallel as NativeDDP\n",
        "from timm.data import create_dataset, create_loader, resolve_data_config,\\\n",
        "      Mixup, FastCollateMixup, AugMixDataset\n",
        "from timm.models import create_model, safe_model_name, convert_splitbn_model,\\\n",
        "      model_parameters\n",
        "from timm.utils import *\n",
        "from timm.loss import *\n",
        "from timm.optim import create_optimizer_v2, optimizer_kwargs\n",
        "from timm.scheduler import create_scheduler\n",
        "from timm.utils import ApexScaler, NativeScaler\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "_logger = logging.getLogger('train')\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### PARSE INPUT ARGUMENTS\n",
        "################################################################################\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "# Setup of the execution\n",
        "parser.add_argument('--seed', '-s', type=int, default=42, metavar='S',\n",
        "                    help='random seed')\n",
        "parser.add_argument('--workers', '-w', type=int, default=2, metavar='WORKERS',\n",
        "                    help='how many training processes to use')\n",
        "parser.add_argument('--eval-metric', default='top1', type=str, \n",
        "                    metavar='EVAL_METRIC', help='Best evaluation metric')\n",
        "parser.add_argument('--output', default='', type=str, metavar='PATH',\n",
        "                    help='path to output folder (default: current dir)')\n",
        "\n",
        "# Dataset Arguments:\n",
        "parser.add_argument('data_dir', metavar='DIR', help='path to dataset')\n",
        "parser.add_argument('--dataset', '-d', metavar='DATA', default='torch/cifar100',\n",
        "                    help='dataset (default: torch/cifar100)')\n",
        "parser.add_argument('--train-split', metavar='TRAIN', default='train',\n",
        "                    help='dataset train split (default: train)')\n",
        "parser.add_argument('--val-split', metavar='VAL', default='validation',\n",
        "                    help='dataset validation split (default: validation)')\n",
        "parser.add_argument('--dataset-download', action='store_true', default=False,\n",
        "                    help='Allow download of dataset for torch/* .')\n",
        "parser.add_argument('--input-size', '-i', default=None, nargs=3, type=int,\n",
        "                    metavar='SIZE', help='''image dimensions (d h w, e.g. \n",
        "                    --input-size 3 224 224)''')\n",
        "parser.add_argument('--num-classes', '-n', type=int, default=None,metavar='NUM',\n",
        "                    help='number of label classes')\n",
        "parser.add_argument('--batch-size', '-b', type=int, default=32, metavar='BATCH',\n",
        "                    help='Training batch size')\n",
        "\n",
        "# Model Arguments\n",
        "parser.add_argument('--model', '-m', default='efficientnetv2_rw_s', type=str, \n",
        "                    metavar='MODEL', help='Name of model to train')\n",
        "parser.add_argument('--pretrained', action='store_true', default=False,\n",
        "                    help='Load pretrained model')\n",
        "parser.add_argument('--global_pool', '-g', default=None, type=str, \n",
        "                    metavar='POOL', help='''Global pool type, one of: (fast, \n",
        "                    avg, max, avgmax, avgmaxc); Model default if None.''')\n",
        "parser.add_argument('--opt', '-o', default='sgd', type=str, metavar='OPTIMIZER',\n",
        "                    help='Optimization method')\n",
        "\n",
        "# Augmentation Arguments\n",
        "parser.add_argument('--no-aug', action='store_true', default=False,\n",
        "                    help='Disable all training augmentation')\n",
        "parser.add_argument('--scale', type=float, nargs='+', default=[0.08, 1.0], \n",
        "                    metavar='PCT', help='Random resize scale')\n",
        "parser.add_argument('--ratio', type=float, nargs='+', default=[3./4., 4./3.], \n",
        "                    metavar='RATIO', help='Random resize aspect ratio')\n",
        "parser.add_argument('--hflip', type=float, default=0.5,\n",
        "                    help='Horizontal flip training aug probability')\n",
        "parser.add_argument('--vflip', type=float, default=0.,\n",
        "                    help='Vertical flip training aug probability')\n",
        "parser.add_argument('--color-jitter', type=float, default=0.4, metavar='PCT',\n",
        "                    help='Color jitter factor')    \n",
        "parser.add_argument('--aa', type=str, default=None, metavar='AUTOAUG',\n",
        "                    help='Use AutoAugment policy: \"v0\" or \"original\"')\n",
        "parser.add_argument('--aug-splits', type=int, default=0,\n",
        "                    help='Number of augmentation splits: 0 or >=2')\n",
        "\n",
        "# Regularization Arguments\n",
        "parser.add_argument('--bce-loss', action='store_true', default=False,\n",
        "                    help='Enable BCE loss w/ Mixup/CutMix use.')\n",
        "parser.add_argument('--mixup', type=float, default=0.0,\n",
        "                    help='mixup alpha, mixup enabled if > 0.0')\n",
        "parser.add_argument('--cutmix', type=float, default=0.0,\n",
        "                    help='cutmix alpha, cutmix enabled if > 0.0')\n",
        "parser.add_argument('--cutmix-minmax', type=float, nargs='+', default=None,\n",
        "                    help='''cutmix min/max ratio, overrides alpha and enables \n",
        "                    cutmix if set''')\n",
        "parser.add_argument('--mixup-prob', type=float, default=1.0,\n",
        "                    help='''Probability of performing mixup or cutmix when \n",
        "                    either/both is enabled''')\n",
        "parser.add_argument('--mixup-switch-prob', type=float, default=0.5,\n",
        "                    help='''Probability of switching to cutmix when both mixup \n",
        "                    and cutmix enabled''')\n",
        "parser.add_argument('--mixup-mode', type=str, default='batch',\n",
        "                    help='''How to apply mixup/cutmix params. Per \"batch\", \n",
        "                    \"pair\", or \"elem\"''')\n",
        "parser.add_argument('--smoothing',type=float,default=0.1,help='Label smoothing')\n",
        "parser.add_argument('--drop', type=float, default=0.0, metavar='DROP',\n",
        "                    help='Dropout rate')\n",
        "\n",
        "\n",
        "# Learning rate Arguments \n",
        "parser.add_argument('--lr', '-l', type=float, default=0.05, metavar='LR',\n",
        "                    help='learning rate')\n",
        "parser.add_argument('--sched', default='cosine', type=str, metavar='SCHEDULER',\n",
        "                    help='Learning rate scheduler')\n",
        "parser.add_argument('--lr-noise', type=float, nargs='+', default=None, \n",
        "                    metavar='NOISE', help='learning rate noise')\n",
        "parser.add_argument('--lr-noise-pct', type=float, default=0.67, \n",
        "                    metavar='PERCENT', help='learning rate noise limit percent')\n",
        "parser.add_argument('--lr-noise-std', type=float, default=1.0, metavar='STDDEV',\n",
        "                    help='learning rate noise std-dev')\n",
        "parser.add_argument('--lr-cycle-mul', type=float, default=1.0, metavar='MULT',\n",
        "                    help='learning rate cycle len multiplier')\n",
        "parser.add_argument('--lr-cycle-decay', type=float, default=0.5,metavar='DECAY',\n",
        "                    help='amount to decay each learning rate cycle')\n",
        "parser.add_argument('--lr-cycle-limit', type=int, default=1, metavar='LIMIT',\n",
        "                    help='learning rate cycle limit, cycles enabled if > 1')\n",
        "parser.add_argument('--lr-k-decay', type=float, default=1.0,\n",
        "                    help='learning rate k-decay for cosine/poly')\n",
        "parser.add_argument('--warmup-lr', type=float, default=0.0001, metavar='WARM',\n",
        "                    help='warmup learning rate')\n",
        "parser.add_argument('--min-lr', type=float, default=1e-6, metavar='MIN',\n",
        "                    help='lower lr bound for cyclic schedulers that hit 0')\n",
        "\n",
        "# Epoch Arguments\n",
        "parser.add_argument('--epochs', '-e', type=int, default=100, metavar='EPOCHS',\n",
        "                    help='number of epochs to train')\n",
        "parser.add_argument('--epoch-repeats', type=float, default=0.0,metavar='REPEAT',\n",
        "                    help='epoch repeat multiplier.')\n",
        "parser.add_argument('--decay-epochs', type=float, default=100, metavar='DLRE',\n",
        "                    help='epoch interval to decay LR')\n",
        "parser.add_argument('--warmup-epochs', type=int, default=3, metavar='WARMEPOCH',\n",
        "                    help='epochs to warmup LR, if scheduler supports')\n",
        "parser.add_argument('--cooldown-epochs', type=int, default=10, metavar='COOL',\n",
        "                    help='epochs to cooldown LR at min_lr, after cyclic ends')\n",
        "parser.add_argument('--patience-epochs', type=int, default=10, \n",
        "                    metavar='PATIENCE', help='patience epochs for LR scheduler')\n",
        "parser.add_argument('--decay-rate', '--dr', type=float, default=0.1, \n",
        "                    metavar='RATE', help='LR decay rate')\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### GENERATE DATASETS\n",
        "################################################################################\n",
        "\n",
        "def get_datasets(args):\n",
        "    train_data = create_dataset(args.dataset, root = args.data_dir, \n",
        "        split=args.train_split, is_training=True,download=args.dataset_download,\n",
        "        batch_size = args.batch_size, repeats = args.epoch_repeats)\n",
        "    test_data = create_dataset(args.dataset, root = args.data_dir, \n",
        "        split=args.val_split, is_training=False, download=args.dataset_download,\n",
        "        batch_size = args.batch_size)\n",
        "    return train_data, test_data\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### CREATE DATA LOADERS\n",
        "################################################################################\n",
        "\n",
        "def get_loaders(train_data, test_data, args, config, num_aug, collate_fn):\n",
        "    interp = 'random'\n",
        "    if args.no_aug or not interp:\n",
        "        interp = config['interpolation']\n",
        "\n",
        "    train_loader = create_loader(train_data, \n",
        "        input_size = config['input_size'], batch_size = args.batch_size,\n",
        "        is_training = True, use_prefetcher = args.prefetcher,\n",
        "        no_aug = args.no_aug, re_prob = 0.0, re_mode = 'pixel', re_count = 1,\n",
        "        re_split = False, scale = args.scale, ratio = args.ratio,\n",
        "        hflip = args.hflip, vflip = args.vflip,color_jitter = args.color_jitter,\n",
        "        auto_augment = args.aa, num_aug_repeats = 0,\n",
        "        num_aug_splits = num_aug, interpolation = interp, mean = config['mean'],\n",
        "        std = config['std'], num_workers = args.workers, distributed = False, \n",
        "        collate_fn = collate_fn, pin_memory = False, \n",
        "        use_multi_epochs_loader = False, worker_seeding = 'all')\n",
        "    \n",
        "    test_loader = create_loader(test_data, input_size = config['input_size'],\n",
        "        batch_size = args.batch_size, is_training = False,\n",
        "        use_prefetcher = args.prefetcher,\n",
        "        interpolation = config['interpolation'], mean = config['mean'],\n",
        "        std = config['std'], num_workers = args.workers, distributed = False, \n",
        "        crop_pct = config['crop_pct'], pin_memory = False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### MAIN METHOD\n",
        "################################################################################\n",
        "\n",
        "def main():\n",
        "    # Initialization of parameters\n",
        "    setup_default_logging()\n",
        "    args = parser.parse_args()\n",
        "    args.mean, args.std, args.crop_pct, args.interpolation = None, None, None,''\n",
        "    args.opt_eps, args.opt_betas, args.layer_decay = None, None, None\n",
        "    args.momentum, args.weight_decay, args.log_interval = 0.9, 2e-5, 50\n",
        "    args.prefetcher, args.device, args.world_size, args.rank = True,'cuda:0',1,0 \n",
        "    _logger.info('Training with a single process on 1 GPU.')\n",
        "    random_seed(args.seed, args.rank)\n",
        "    train_data, test_data = get_datasets(args)\n",
        "    \n",
        "    # Generating the model\n",
        "    model = create_model(args.model, pretrained = args.pretrained,\n",
        "        num_classes = args.num_classes, drop_rate = args.drop,\n",
        "        drop_connect_rate = None,  drop_path_rate = None,\n",
        "        drop_block_rate = None, global_pool = args.global_pool,\n",
        "        bn_momentum = None, bn_eps = None, scriptable = None)\n",
        "    model.cuda()\n",
        "    optimizer = create_optimizer_v2(model, **optimizer_kwargs(cfg=args))\n",
        "    if args.num_classes is None:\n",
        "        assert hasattr(model, 'num_classes')\n",
        "        args.num_classes = model.num_classes \n",
        "    _logger.info(f'Model name: {safe_model_name(args.model)}')\n",
        "    num_params = sum([m.numel() for m in model.parameters()])\n",
        "    _logger.info(f'Number of Parameters: {num_params}')\n",
        "    \n",
        "    # Setup learning rate scheduler\n",
        "    lr_sched, num_epochs = create_scheduler(args, optimizer)\n",
        "    _logger.info('Scheduled epochs: {}'.format(num_epochs))\n",
        "\n",
        "    # Setup data augmentation \n",
        "    num_aug_splits = 0\n",
        "    if args.aug_splits > 0:\n",
        "        assert args.aug_splits > 1, 'A split of 1 makes no sense'\n",
        "        num_aug_splits = args.aug_splits\n",
        "    collate_fn, mixup_fn = None, None\n",
        "    mixup_flag = args.mixup > 0 or args.cutmix > 0.0\n",
        "    mixup_active = mixup_flag or args.cutmix_minmax is not None\n",
        "    if mixup_active:\n",
        "        mixup_args = dict(mixup_alpha = args.mixup, cutmix_alpha = args.cutmix, \n",
        "            cutmix_minmax = args.cutmix_minmax, prob = args.mixup_prob, \n",
        "            switch_prob = args.mixup_switch_prob, mode = args.mixup_mode,\n",
        "            label_smoothing = args.smoothing, num_classes = args.num_classes)\n",
        "        if args.prefetcher:\n",
        "            assert not num_aug_splits  \n",
        "            collate_fn = FastCollateMixup(**mixup_args)\n",
        "        else:\n",
        "            mixup_fn = Mixup(**mixup_args)\n",
        "    if num_aug_splits > 1:\n",
        "        train_data = AugMixDataset(train_data, num_splits = num_aug_splits)\n",
        "\n",
        "    # Generate data loaders (w/ augmentation)\n",
        "    data_config = resolve_data_config(vars(args), model = model, verbose = True)   \n",
        "    train_loader, test_loader = get_loaders(train_data, test_data, args, \n",
        "                                data_config, num_aug_splits, collate_fn)\n",
        "    \n",
        "    # Setup loss functions\n",
        "    if mixup_active:\n",
        "        if args.bce_loss:\n",
        "            train_loss_fn = BinaryCrossEntropy(target_threshold = None)\n",
        "        else:\n",
        "            train_loss_fn = SoftTargetCrossEntropy()\n",
        "    elif args.smoothing:\n",
        "        if args.bce_loss:\n",
        "            train_loss_fn = BinaryCrossEntropy(smoothing = args.smoothing, \n",
        "                                                  target_threshold = None)\n",
        "        else:\n",
        "            train_loss_fn = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n",
        "    else:\n",
        "        train_loss_fn = nn.CrossEntropyLoss()\n",
        "    train_loss_fn = train_loss_fn.cuda()\n",
        "    test_loss_fn = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    # Setup train and test metrics\n",
        "    eval_metric = args.eval_metric\n",
        "    saver, output_dir = None, None\n",
        "    if args.rank == 0:\n",
        "        exp_name = '-'.join([datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        "            safe_model_name(args.model), str(data_config['input_size'][-1])])\n",
        "        output_dir =get_outdir(args.output if args.output else './output/train', \n",
        "                               exp_name)\n",
        "        decreasing = True if eval_metric == 'loss' else False\n",
        "        saver = CheckpointSaver(model = model, optimizer = optimizer,args =args,\n",
        "                model_ema = None, amp_scaler = None, checkpoint_dir =output_dir,\n",
        "                recovery_dir = output_dir, decreasing = decreasing, \n",
        "                max_history = 10)\n",
        "\n",
        "    try:\n",
        "        loss, top_1, top_5 = [], [], []\n",
        "        t_init = time.time()\n",
        "        for epoch in range(num_epochs):\n",
        "            train_metrics = train_one_epoch(epoch, model, train_loader, \n",
        "                            optimizer, train_loss_fn, args, lr_sched = lr_sched,\n",
        "                            saver = saver, output_dir = output_dir,\n",
        "                            loss_scaler = None, model_ema = None, \n",
        "                            mixup_fn = mixup_fn)\n",
        "            loss.append(train_metrics['loss'])\n",
        "            \n",
        "            test_metrics = validate(model, test_loader, test_loss_fn, args)\n",
        "            top_1.append(test_metrics['top1'])\n",
        "            top_5.append(test_metrics['top5'])\n",
        "            if lr_sched is not None:\n",
        "                lr_sched.step(epoch + 1, test_metrics[eval_metric])\n",
        "            if output_dir is not None:\n",
        "                update_summary(epoch, train_metrics, test_metrics, \n",
        "                    os.path.join(output_dir, 'summary.csv'),\n",
        "                    write_header = True, log_wandb = False)\n",
        "        t_end = time.time()\n",
        "        print(\"Total execution time: \", t_end - t_init)\n",
        "        plot_figures(loss, top_1, top_5)\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### TRAINING METHOD\n",
        "################################################################################\n",
        "\n",
        "def train_one_epoch(epoch, model, loader, optimizer, loss_fn, args,\n",
        "        lr_sched = None, saver = None, output_dir = None,\n",
        "        loss_scaler = None, model_ema = None, mixup_fn = None):\n",
        "    batch_time_m, data_time_m = AverageMeter(), AverageMeter()\n",
        "    losses_m = AverageMeter()\n",
        "    model.train()\n",
        "    end = time.time()\n",
        "    last_idx = len(loader) - 1\n",
        "    num_updates = epoch * len(loader)\n",
        "\n",
        "    # Load the data batch\n",
        "    for batch_idx, (input, target) in enumerate(loader):\n",
        "        last_batch = batch_idx == last_idx\n",
        "        data_time_m.update(time.time() - end)\n",
        "        if not args.prefetcher:\n",
        "            input, target = input.cuda(), target.cuda()\n",
        "            if mixup_fn is not None:\n",
        "                input, target = mixup_fn(input, target)\n",
        "        \n",
        "        # Forward propagate the network\n",
        "        with suppress():\n",
        "            output = model(input)\n",
        "            loss = loss_fn(output, target)\n",
        "        losses_m.update(loss.item(), input.size(0))\n",
        "\n",
        "        # Backward propagate the network\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        num_updates += 1\n",
        "        batch_time_m.update(time.time() - end)\n",
        "        if last_batch or batch_idx % args.log_interval == 0:\n",
        "            lrl = [param_group['lr'] for param_group in optimizer.param_groups]\n",
        "            lr = sum(lrl) / len(lrl) \n",
        "\n",
        "            # Record Results      \n",
        "            _logger.info('Train: {} [{:>4d}/{} ({:>3.0f}%)]  '\n",
        "                    'Loss: {loss.val:#.4g} ({loss.avg:#.3g})  '\n",
        "                    'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s  '\n",
        "                    '({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '\n",
        "                    'LR: {lr:.3e}  '\n",
        "                    'Data: {data_time.val:.3f} ({data_time.avg:.3f})'.format(\n",
        "                    epoch, batch_idx, len(loader), 100. * batch_idx / last_idx,\n",
        "                    loss = losses_m, batch_time = batch_time_m,\n",
        "                    rate = input.size(0) * args.world_size / batch_time_m.val,\n",
        "                    rate_avg=input.size(0) * args.world_size / batch_time_m.avg,\n",
        "                    lr = lr, data_time = data_time_m))\n",
        "        if lr_sched is not None:\n",
        "            lr_sched.step_update(num_updates = num_updates, metric=losses_m.avg)\n",
        "        end = time.time()\n",
        "        \n",
        "    if hasattr(optimizer, 'sync_lookahead'):\n",
        "        optimizer.sync_lookahead()\n",
        "    return OrderedDict([('loss', losses_m.avg)])\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### EVALUATION METHOD\n",
        "################################################################################\n",
        "\n",
        "def validate(model, loader, loss_fn, args, log_suffix=''):\n",
        "    batch_time_m, losses_m = AverageMeter(), AverageMeter()\n",
        "    top1_m, top5_m = AverageMeter(), AverageMeter()\n",
        "    model.eval()\n",
        "    end = time.time()\n",
        "    last_idx = len(loader) - 1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (input, target) in enumerate(loader):\n",
        "            last_batch = batch_idx == last_idx\n",
        "            if not args.prefetcher:\n",
        "                input = input.cuda()\n",
        "                target = target.cuda()\n",
        "            \n",
        "            # Predict test data\n",
        "            with suppress():\n",
        "                output = model(input)\n",
        "            if isinstance(output, (tuple, list)):\n",
        "                output = output[0]\n",
        "\n",
        "            # Compute loss and accuracy of predictions\n",
        "            loss = loss_fn(output, target)\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            reduced_loss = loss.data\n",
        "            torch.cuda.synchronize()\n",
        "            losses_m.update(reduced_loss.item(), input.size(0))\n",
        "            top1_m.update(acc1.item(), output.size(0))\n",
        "            top5_m.update(acc5.item(), output.size(0))\n",
        "\n",
        "            batch_time_m.update(time.time() - end)\n",
        "            end = time.time()\n",
        "            if (last_batch or batch_idx % args.log_interval == 0):\n",
        "                log_name = 'Test' + log_suffix\n",
        "                _logger.info('{0}: [{1:>4d}/{2}]  '\n",
        "                    'Time: {batch_time.val:.3f} ({batch_time.avg:.3f})  '\n",
        "                    'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  '\n",
        "                    'Acc@1: {top1.val:>7.4f} ({top1.avg:>7.4f})  '\n",
        "                    'Acc@5: {top5.val:>7.4f} ({top5.avg:>7.4f})'.format(\n",
        "                    log_name, batch_idx, last_idx, batch_time = batch_time_m,\n",
        "                    loss = losses_m, top1 = top1_m, top5 = top5_m))\n",
        "\n",
        "    metrics = OrderedDict([('loss', losses_m.avg), ('top1', top1_m.avg), \n",
        "                           ('top5', top5_m.avg)])\n",
        "    return metrics\n",
        "\n",
        "\n",
        "################################################################################\n",
        "### PLOTTING RESULTS\n",
        "################################################################################\n",
        "\n",
        "def plot_figures(loss, top_1, top_5):\n",
        "    num_epochs = len(loss)\n",
        "    epochs = [i for i in range(num_epochs)]\n",
        "    loss_fig = plt.figure()\n",
        "    plt.plot(epochs, loss)\n",
        "    plt.xlim((0, num_epochs))\n",
        "    plt.grid(True)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Training loss')\n",
        "    loss_fig.savefig('output/train/loss.png', format='png')\n",
        "    \n",
        "    top1_fig = plt.figure()\n",
        "    plt.plot(epochs, top_1)\n",
        "    plt.xlim((0, num_epochs))\n",
        "    plt.grid(True)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Test top_1 accuracy')\n",
        "    top1_fig.savefig('output/train/top1.png', format='png')\n",
        "    \n",
        "    top5_fig = plt.figure()\n",
        "    plt.plot(epochs, top_5)\n",
        "    plt.xlim((0, num_epochs))\n",
        "    plt.grid(True)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Test top_5 accuracy')\n",
        "    top5_fig.savefig('output/train/top5.png', format='png')\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "################################################################################\n",
        "### GLOBAL METHODS AND VARIABLES\n",
        "################################################################################\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brnJEQbySX9Q",
        "outputId": "f70a6e30-c05d-4bdd-d6fb-f3138c1ed362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run train.py -d torch/cifar10 --dataset-download datasets --model efficientnet_b1 --epochs 2 -b 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrgg9dd42BPd",
        "outputId": "31a4c0b4-8f72-44bc-8013-5a00a05ba626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training with a single process on 1 GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model name: efficientnet_b1\n",
            "Number of Parameters: 7794184\n",
            "Scheduled epochs: 12\n",
            "Data processing configuration for current model + dataset:\n",
            "\tinput_size: (3, 224, 224)\n",
            "\tinterpolation: bicubic\n",
            "\tmean: (0.485, 0.456, 0.406)\n",
            "\tstd: (0.229, 0.224, 0.225)\n",
            "\tcrop_pct: 1.0\n",
            "Train: 0 [   0/6250 (  0%)]  Loss: 7.035 (7.03)  Time: 2.072s,    3.86/s  (2.072s,    3.86/s)  LR: 1.000e-04  Data: 0.170 (0.170)\n",
            "Train: 0 [  50/6250 (  1%)]  Loss: 6.790 (6.91)  Time: 0.244s,   32.81/s  (0.263s,   30.39/s)  LR: 1.000e-04  Data: 0.010 (0.015)\n"
          ]
        }
      ]
    }
  ]
}